{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ftfy import fix_text\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "DATA_DIR2 = DATA_DIR\n",
    "\n",
    "# DATA_DIR = \"/kaggle/input/es-tw-dataset/\"\n",
    "# DATA_DIR2 = \"/kaggle/working\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    !mkdir {DATA_DIR}\n",
    "\n",
    "    # Data sourced from here: https://opus.nlpl.eu/results/en&tw/corpus-result-table\n",
    "    url = \"https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/en-tw.txt.zip\"\n",
    "\n",
    "    !wget {url} -O {DATA_DIR}/en-tw.txt.zip\n",
    "    !unzip -j {DATA_DIR}/en-tw.txt.zip -d {DATA_DIR}/ '*.tw' '*.en'\n",
    "    !rm {DATA_DIR}/en-tw.txt.zip\n",
    "    !cat {DATA_DIR}/NLLB.en-tw.en >> {DATA_DIR}/eng.txt && cat {DATA_DIR}/NLLB.en-tw.tw >> {DATA_DIR}/twi.txt\n",
    "    !rm {DATA_DIR}/*en {DATA_DIR}/*tw\n",
    "\n",
    "if not os.path.exists(DATA_DIR2):\n",
    "    !mkdir {DATA_DIR2}\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "    !mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def translate_data(lines, model_name=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\"):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    inputs = tokenizer(lines, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    translated_fixed = [fix_text(text) for text in translated]\n",
    "\n",
    "    return translated\n",
    "\n",
    "def translate_save_data(input_file, output_file, batch_size=100):\n",
    "    if not os.path.exists(output_file):\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            with open(input_file, 'r', encoding='utf-8') as in_file, \\\n",
    "                open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "\n",
    "                lines = in_file.readlines()\n",
    "                total_lines = len(lines)\n",
    "\n",
    "                for i in tqdm(range(0, total_lines, batch_size), desc=\"Translating\", unit=\"batch\"):\n",
    "                    batch = [line.strip() for line in lines[i:i+batch_size]]\n",
    "                    transations = translate(batch)\n",
    "                    for translation in translations:\n",
    "                        out_file.write(translation + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def clean_data(input_file_lang1, input_file_lang2, output_file_lang1, output_file_lang2):\n",
    "    pattern = r'[^\\wáéíóúüÁÉÍÓÚÜ\\s?!.¿¡\\'\\:]'\n",
    "    seen_lines = set()\n",
    "    \n",
    "    with open(input_file_lang1, 'r', encoding='utf-8') as f_input_lang1, \\\n",
    "         open(input_file_lang2, 'r', encoding='utf-8') as f_input_lang2, \\\n",
    "         open(output_file_lang1, 'w', encoding='utf-8') as f_output_lang1, \\\n",
    "         open(output_file_lang2, 'w', encoding='utf-8') as f_output_lang2:\n",
    "        \n",
    "        for line1, line2 in zip(f_input_lang1, f_input_lang2):\n",
    "            clean_line1 = re.sub(pattern, '', line1.strip()).strip().lower()\n",
    "            clean_line2 = re.sub(pattern, '', line2.strip()).strip().lower()\n",
    "            \n",
    "            clean_line1 = ' '.join(word for word in clean_line1.split() if not any(char.isdigit() for char in word))\n",
    "            clean_line2 = ' '.join(word for word in clean_line2.split() if not any(char.isdigit() for char in word))\n",
    "            \n",
    "            if len(clean_line1) < 5 or len(clean_line2) < 5:\n",
    "                continue\n",
    "            \n",
    "            if (clean_line1, clean_line2) in seen_lines:\n",
    "                continue\n",
    "                \n",
    "            seen_lines.add((clean_line1, clean_line2))\n",
    "            f_output_lang1.write(clean_line1 + '\\n')\n",
    "            f_output_lang2.write(clean_line2 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# translate_save_data(f'{DATA_DIR}/eng.txt', f'{DATA_DIR}/esp.txt')  # 2,5 hours for 100,000 sentences\n",
    "# clean_data(f'{DATA_DIR}/esp.txt', f'{DATA_DIR}/twi.txt', f'{DATA_DIR2}/esp2.txt', f'{DATA_DIR2}/twi2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "START_TOKEN = '<sos>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "END_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "special_tokens = [START_TOKEN, PADDING_TOKEN, END_TOKEN, UNK_TOKEN]\n",
    "\n",
    "esp_vocab = special_tokens + list(\"aábcdeéfghiíjklmnñoópqrstuúüvwxyz.,¿?¡!': \")\n",
    "twi_vocab = special_tokens + list(\"abcdeɛfghijklmnoɔpqrstuvwxyz.,?!': \")\n",
    "\n",
    "index_to_esp = {k:v for k,v in enumerate(esp_vocab)}\n",
    "esp_to_index = {v:k for k,v in enumerate(esp_vocab)}\n",
    "index_to_twi = {k:v for k,v in enumerate(twi_vocab)}\n",
    "twi_to_index = {v:k for k,v in enumerate(twi_vocab)}\n",
    "\n",
    "def encode(line, lang, max_len):\n",
    "    result = [lang[START_TOKEN]]\n",
    "    for char in line:\n",
    "        try:\n",
    "            result.append(lang[char])\n",
    "        except:\n",
    "            result.append(lang[UNK_TOKEN])\n",
    "\n",
    "    if len(result) >= max_len:\n",
    "        result = result[:max_len - 1]\n",
    "        result.append(lang[END_TOKEN])\n",
    "    else:\n",
    "        result.append(lang[END_TOKEN])\n",
    "        result += [lang[PADDING_TOKEN]] * (max_len - len(line) - 2)\n",
    "\n",
    "    return result\n",
    "\n",
    "def decode(line, lang, skip_special_tokens=False):\n",
    "    if not skip_special_tokens:\n",
    "        return ''.join([lang[char] for char in line])\n",
    "    else:\n",
    "        return ''.join([lang[char] for char in line if lang[char] not in special_tokens])\n",
    "\n",
    "print(index_to_esp)\n",
    "print(index_to_twi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_instances = 85326\n",
    "\n",
    "with open(f'{DATA_DIR2}/esp2.txt', 'r', encoding='utf-8') as file_esp, open(f'{DATA_DIR2}/twi2.txt', 'r', encoding='utf-8') as file_twi:\n",
    "    x = [line.rstrip('\\n') for line in file_esp.readlines()[:n_instances]]\n",
    "    y = [line.rstrip('\\n') for line in file_twi.readlines()[:n_instances]]\n",
    "\n",
    "len_esp = [len(line) for line in x]\n",
    "len_twi = [len(line) for line in y]\n",
    "\n",
    "print(f\"Dataset size: {n_instances} lines\")\n",
    "print(f\"Spanish vocabulary size: {len(esp_vocab)} characters\")\n",
    "print(f\"Twi vocabulary size: {len(twi_vocab)} characters\\n\")\n",
    "\n",
    "percentile = 99\n",
    "\n",
    "print(f\"Average length of Spanish sentences: {int(np.mean(len_esp))} characters\")\n",
    "print(f\"Average length of Twi sentences: {int(np.mean(len_twi))} characters\\n\")\n",
    "\n",
    "print(f\"{percentile}th percentile of Spanish sentence lengths:\", int(np.percentile(len_esp, percentile)))\n",
    "print(f\"{percentile}th percentile of Twi sentence lengths:\", int(np.percentile(len_twi, percentile)), '\\n')\n",
    "\n",
    "len_lengths_list = 40\n",
    "\n",
    "top_esp = sorted(len_esp, reverse=True)[:len_lengths_list]\n",
    "top_twi = sorted(len_twi, reverse=True)[:len_lengths_list]\n",
    "\n",
    "print(f\"Top {len_lengths_list} longest line lengths for Spanish:\")\n",
    "print(top_esp)\n",
    "\n",
    "print(f\"\\nTop {len_lengths_list} longest line lengths for Twi:\")\n",
    "print(top_twi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        src_pad_idx,\n",
    "        tgt_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_word_embedding = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dim_feedforward=forward_expansion,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src == self.src_pad_idx)\n",
    "        return src_mask.to(self.device).bool()\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        tgt_mask = (tgt == self.tgt_pad_idx)\n",
    "        return tgt_mask.to(self.device).bool()\n",
    "\n",
    "    def positional_encoding(self, seq_length, embedding_size):\n",
    "        pe = torch.zeros(seq_length, embedding_size, device=self.device)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float, device=self.device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2, dtype=torch.float, device=self.device) * (-math.log(10000.0) / embedding_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  \n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size, src_seq_length = src.shape\n",
    "        batch_size, tgt_seq_length = tgt.shape\n",
    "\n",
    "        embed_src = self.src_word_embedding(src) + self.positional_encoding(src_seq_length, embedding_size)\n",
    "        embed_tgt = self.tgt_word_embedding(tgt) + self.positional_encoding(tgt_seq_length, embedding_size)\n",
    "\n",
    "        embed_src = self.dropout(embed_src)\n",
    "        embed_tgt = self.dropout(embed_tgt)\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        tgt_padding_mask = self.make_tgt_mask(tgt)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_seq_length, tgt_seq_length, dtype=torch.bool, device=self.device), diagonal=1)\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "src_vocab_size = len(esp_vocab)\n",
    "tgt_vocab_size = len(twi_vocab)\n",
    "max_len = 250\n",
    "\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "forward_expansion = 2048\n",
    "src_pad_idx = esp_to_index[PADDING_TOKEN]\n",
    "tgt_pad_idx = twi_to_index[PADDING_TOKEN]\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    src_pad_idx,\n",
    "    tgt_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "num_epochs = 55\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class EspTwiDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_lang, tgt_lang, max_len):\n",
    "        self.src_lines = list(map(lambda x: encode(x, src_lang, max_len), src_lines))\n",
    "        self.tgt_lines = list(map(lambda x: encode(x, tgt_lang, max_len), tgt_lines))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_lines[idx]), torch.tensor(self.tgt_lines[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset = EspTwiDataset(x, y, esp_to_index, twi_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, tmp_indices = train_test_split(indices, train_size=0.9)\n",
    "dev_indices, test_indices = train_test_split(tmp_indices, train_size=0.98)\n",
    "\n",
    "train_set = Subset(dataset, train_indices)\n",
    "dev_set = Subset(dataset, dev_indices)\n",
    "test_set = Subset(dataset, test_indices)\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "dev_dl = DataLoader(dev_set, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"train set size: {len(train_set)}\")\n",
    "print(f\"dev set size: {len(dev_set)}\")\n",
    "print(f\"test set size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in (t := trange(len(dataloader), desc=\"training\", unit=\"batch\")):\n",
    "        src, tgt = next(iter(dataloader))\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])  \n",
    "        output = output.view(-1, output.size(-1))\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t.set_description(f\"training loss: {loss.item():.10f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_model(model, dev_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(), trange(len(dev_loader), desc=\"validation\", unit=\"batch\") as t:\n",
    "        for src, tgt in dev_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            t.set_description(f\"validation loss: {loss.item():.10f}\")\n",
    "\n",
    "    return total_loss / len(dev_loader)\n",
    "\n",
    "\n",
    "def train_loop(model, train_loader, dev_loader, optimizer, criterion, device, num_epochs):\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    train_info = f\"NUM_EPOCHS={num_epochs}   BATCH_SIZE={batch_size}   LEARNING_RATE={learning_rate}   DROPOUT={dropout}\"\n",
    "\n",
    "    print(train_info)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEPOCH {epoch + 1}/{num_epochs}\")\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if epoch != 0:\n",
    "            signo = f\"+\" if train_losses[-1] > train_losses[-2] else \"\"\n",
    "            print(f\"average train loss for epoch {epoch + 1}: {train_loss:.10f} ({signo}{(train_losses[-1] - train_losses[-2]):.10f})\")\n",
    "        else: \n",
    "            print(f\"average train loss for epoch {epoch + 1}: {train_loss:.10f}\")\n",
    "        \n",
    "        dev_loss = validate_model(model, dev_loader, criterion, device)\n",
    "        dev_losses.append(dev_loss)\n",
    "\n",
    "        if epoch != 0:\n",
    "            signo = f\"+\" if dev_losses[-1] > dev_losses[-2] else \"\"\n",
    "            print(f\"average val loss for epoch {epoch + 1}: {dev_loss:.10f} ({signo}{(dev_losses[-1] - dev_losses[-2]):.10f})\\n\")\n",
    "        else: \n",
    "            print(f\"average val loss for epoch {epoch + 1}: {dev_loss:.10f}\\n\")\n",
    "\n",
    "        scheduler.step(dev_loss)\n",
    "        torch.save(model.state_dict(), f'models/e{num_epochs}_b{batch_size}.pt')\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(dev_losses, label='Validation Loss', color='green')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Validation Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = train_loop(model, train_dl, dev_dl, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(f'models/e{num_epochs}_b{batch_size}.pt'))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate(src, model, skip_special_tokens=False):\n",
    "    src_ids = torch.tensor([encode(src, esp_to_index, max_len)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tgt_ids = torch.tensor([twi_to_index[START_TOKEN]]).unsqueeze(1).to(device)\n",
    "\n",
    "        while True:\n",
    "            out = model(src_ids, tgt_ids)\n",
    "            predicted_index = out.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
    "            tgt_ids = torch.cat((tgt_ids, predicted_index), dim=1)\n",
    "            \n",
    "            if twi_to_index[END_TOKEN] in predicted_index or len(tgt_ids[0]) >= max_len:\n",
    "                break\n",
    "    \n",
    "    result = tgt_ids\n",
    "    predicted_words = decode(result.squeeze().tolist(), index_to_twi, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "    return result, predicted_words\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, num_phrases_to_print):\n",
    "    model.to(device)\n",
    "\n",
    "    sources = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    phrases_printed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.shape[0]):\n",
    "                src_sentence = decode(src[i].tolist(), index_to_esp, skip_special_tokens=True)\n",
    "                _, prediction = translate(src_sentence, model, skip_special_tokens=True)\n",
    "\n",
    "                try:\n",
    "                    reference = decode(tgt[i].tolist(), index_to_twi, skip_special_tokens=True)\n",
    "                except KeyError as e:\n",
    "                    print(f\"KeyError: {e} in reference\")\n",
    "                    reference = \"\"\n",
    "\n",
    "                sources.append(src_sentence)\n",
    "                predictions.append(prediction)\n",
    "                references.append(reference)\n",
    "\n",
    "        for i in range(min(num_phrases_to_print, len(predictions))):\n",
    "            print(\"\\n---\")\n",
    "            print(f\"Source: {sources[i]}\")\n",
    "            print(f\"Prediction: {predictions[i]}\")\n",
    "            print(f\"Reference: {references[i]}\")\n",
    "            print(\"---\")\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references], max_order=2)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "source = \"si de lo que hay en la tierra os digo y no creéis ¿cómo podríais creerme si os dijera de lo que hay en el cielo?\"\n",
    "# target: \"sɛ meka asase yi so nsɛm kyerɛ mo na munnye nni a ɛbɛyɛ dɛn na sɛ meka ɔsoro nsɛm kyerɛ mo a mubegye adi?\"\n",
    "\n",
    "result, prediction = translate(source, model, skip_special_tokens=True)\n",
    "# Translate the output sentence in Google Translate: https://translate.google.es/?hl=es&sl=ak&tl=es&op=translate\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "bleu_score = test_model(model, test_dl, num_phrases_to_print=3)\n",
    "\n",
    "for key, value in bleu_score.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
